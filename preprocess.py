import os, pefile
from os import listdir
from sklearn.feature_extraction.text import HashingVectorizer, TfidfTransformer
from sklearn.pipeline import Pipeline
from scipy.sparse import hstack, csr_matrix


from ngrams import get_NGram_features_from_sample


def get_list_of_samples(directories_with_labels):
    list_of_samples = []
    labels = []
    for path, label in directories_with_labels:
        samples = [f for f in listdir(path)]
        for sample in samples:
            file_path = os.path.join(path, sample)
            list_of_samples.append(file_path)
            labels.append(label)
    return list_of_samples, labels


def preprocess_imports(list_of_DLLs):
    # normalize the naming of the imports of a PE file
    temp = []
    for x in list_of_DLLs:
        temp.append(x.decode().split(".")[0].lower())
    return " ".join(temp)


def get_imports(pe):
    """Get a list of the imports of a PE file."""
    list_of_imports = []
    if pe.DIRECTORY_ENTRY_IMPORT:
        for entry in pe.DIRECTORY_ENTRY_IMPORT:
            list_of_imports.append(entry.dll)
    return preprocess_imports(list_of_imports)


def get_section_names(pe):
    """Gets a list of section names from a PE file."""
    list_of_section_names = []
    for sec in pe.sections:
        normalized_name = sec.Name.decode().replace("\x00","").lower()
        list_of_section_names.append(normalized_name)
    return "".join(list_of_section_names)

def generate_dataset(samples, labels, K1_most_frequent_Ngrams_list):
    imports_corpus = []
    num_sections = []
    section_names = []
    Ngram_features_list = []
    y = []
    for i in range(len(samples)):
        sample = samples[i]
        try:
            NGram_features = get_NGram_features_from_sample(
                sample, K1_most_frequent_Ngrams_list
            )
            pe = pefile.PE(sample)
            imports = get_imports(pe)
            n_sections = len(pe.sections)
            sec_names = get_section_names(pe)
            imports_corpus.append(imports)
            num_sections.append(n_sections)
            section_names.append(sec_names)
            Ngram_features_list.append(NGram_features)
            y.append(labels[i])
        except Exception as e:
            print(sample + ":")
            print(e)

    # hashing vectorizer
    imports_featurizer = Pipeline([
        ("vect", HashingVectorizer(input="content", ngram_range=(1,2))),
        ("tfidf", TfidfTransformer(use_idf=True,)),])

    section_names_featurizer = Pipeline([
        ("vect", HashingVectorizer(input="content", ngram_range=(1,2))),
        ("tfidf", TfidfTransformer(use_idf=True,)),])

    imports_corpus_train_transformed = imports_featurizer.fit_transform(imports_corpus)

    section_names_train_transformed = section_names_featurizer.fit_transform(section_names)

    # combine to single array
    X = hstack([
        Ngram_features_list,
        imports_corpus_train_transformed,
        section_names_train_transformed,
        csr_matrix(num_sections).transpose(),])

    return X, y



